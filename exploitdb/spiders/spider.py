# -*- coding: utf-8 -*-
import scrapy
from scrapy.http import Request
from exploitdb.items import ExploitdbItem


class SpiderSpider(scrapy.Spider):
    name = 'spider'
    allowed_domains = ['www.exploit-db.com']
    start_urls = ['https://www.exploit-db.com/browse/']
    requestsNum = 0
    # xpath 解析规则, 规则字典中的 key 要和数据库中的字段名匹配
    rules = {
        'title': '//h1[contains(@class,"w-page-title")]/text()',
        'date': (
            'substring-after(//table[@class="exploit_list"]//td[strong[contains(text(),"Published")]]/text() ,":")',
            '//table[@class="exploit_list"]//td[strong[contains(text(),"Published")]]/span/text()'
        ),
        'CVE': '//div[@class="info"]//table[@class="exploit_list"]//td[strong[contains(text(),"CVE")]]/a/text()',
        'EDB_ID': 'substring-after(//div[@class="info"]//table[@class="exploit_list"]//td[strong[contains(text(),"EDB-ID")]]/text(),":")',
        'tags': 'substring-after(//div[@class="info"]//table[@class="exploit_list"]//td[strong[contains(text(),"Tags")]]/text()[2],":")',
        'author': '//div[@class="info"]//table[@class="exploit_list"]//td[strong[contains(text(),"Author")]]/a/text()',
        'type': '//div[@class="info"]//table[@class="exploit_list"]//td[strong[contains(text(),"Type")]]/a/text()',
        'platform': '//div[@class="info"]//table[@class="exploit_list"]//td[strong[contains(text(),"Platform")]]/a/text()',
        'source': '//div[@class="info"]//table[@class="exploit_list"]//td[strong[contains(text(),"Source")]]/a/@href',
    }

    def parse(self, response):
        url = response.css('div.pagination a:last-of-type::attr(href)').extract_first()
        lastEqualSign = url.rindex('=')
        # maxPagination = int(url[lastEqualSign + 1:])  # 爬取所有页码
        maxPagination = 1   # 需要要爬取的页数，如果每天都定时爬的话设为 1 就够了
        for pageination in range(1, maxPagination + 1):
            yield Request(url[:lastEqualSign + 1] + str(pageination), self.getList)
        pass

    def getList(self, response):
        # 用 css选择器来解析 url
        urls = response.css('table.exploit_list tbody tr td.description a::attr(href)').extract()
        for url in urls:
            # 23页有个url是坑爹的pdf文件，所以要检查 url
            if url.startswith('https://www.exploit-db.com/exploits/') and not self.mysqlPipeline.idExist(
                    url.strip('/').rsplit('/')[-1]):
                yield Request(url, self.getDetail)

    def getDetail(self, response):
        result = dict()
        availableKey = []
        for key in self.rules:
            # 规则是字符串则直接解析
            if isinstance(self.rules[key], str):
                text = response.xpath(self.rules[key]).extract_first()
                if text != '' and text is not None:
                    result[key] = text.strip()
                    availableKey.append(key)
            # 规则是tuple则按序解析，解析成功则退出循环，否则继续解析tuple中的下一条规则
            elif isinstance(self.rules[key], tuple):
                for rulerule in self.rules[key]:
                    text = response.xpath(rulerule).extract_first()
                    if text is not None and not self.allAreSpace(text):
                        result[key] = text.strip()
                        availableKey.append(key)
                        break
        result['availableKey'] = availableKey
        self.requestsNum = self.requestsNum + 1
        print(self.requestsNum)
        exploitUrl = response.xpath(
            '(//div[@class="info"]//table[@class="exploit_list"]//td[strong[contains(text(),"Exploit")]]'
            '| //div[@class="info"]//table[@class="exploit_list"]//td[strong[contains(text(),"Shellcode")]])'
            '/a[2]/@href').extract_first()
        yield Request(exploitUrl, self.getExploit, meta=result)     # 注意 meta 只能传递字典，不能传递 item

    def getExploit(self, response):
        item = ExploitdbItem()
        item._values['exploit'] = response.text
        for key in response.meta['availableKey']:
            # item._values[key]相当于 item[key] ,但可以绕过 scrapy 对 key 存在性的检查
            item._values[key] = response.meta[key]
        yield item

    @staticmethod
    def allAreSpace(string):
        for i in string:
            if i != ' ' and i != '\n':
                return False
        return True
